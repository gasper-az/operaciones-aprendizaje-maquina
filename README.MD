# Operaciones de Aprendizaje de MÃ¡quina

## Integrantes

- Acevedo Zain, Gaspar (acevedo.zain.gaspar@gmail.com)
- Chunga, Juan Miguel (juanmiguel.ch2014@gmail.com)
- Gonzalez, Juan (juan.gonzalez.working@gmail.com)
- Lauro, Rodrigo (ing.rodrigo.lauro@gmail.com)
- Rodrigues da Cruz, NicolÃ¡s (Nicolasrdc151@gmail.com)

## DescripciÃ³n

ImplementaciÃ³n de un modelo de Machine Learning (**Body Fat Analysis**) en un entorno productivo simulado con **Docker Compose**.  
El objetivo es desplegar y orquestar un pipeline de MLOps con los siguientes servicios:

- **Apache Airflow**: orquestaciÃ³n de flujos de trabajo (DAGs de entrenamiento y predicciÃ³n).
- **MLflow**: gestiÃ³n del ciclo de vida de modelos (tracking, versionado y registro).
- **FastAPI**: servicio REST para exponer el modelo entrenado.
- **MinIO (S3 compatible)**: almacenamiento de datasets y artefactos de modelos.
- **PostgreSQL**: base de datos backend para MLflow y Airflow.
- **Valkey (Redis fork)**: backend para ejecuciÃ³n distribuida de Airflow.

### CÃ³mo inicializar la aplicaciÃ³n?

***Pre requisitos***

1. `git`.
1. `docker` o `docker desktop`.

***Pasos necesarios***

1. Clonar el repo: `git clone https://github.com/gasper-az/operaciones-aprendizaje-maquina.git`.
1. Una vez descargado, ubicarse en el directorio del repositorio: `cd operaciones-aprendizaje-maquina`.
1. Asegurarse que `docker` (o `docker desktop`) estÃ© inicializado y funcionando correctamente.
1. Ejecutar el comando `docker compose up`.

### Diagrama

![diagrama](./diagrams/arquitectura.drawio.png)

***Referencias***

1. **Redis** se utiliza para configurar **Airflow** al inicializar los servicios.
1. **Apache Airflow** consume el dataset de *BodyFat*, realizando distintas operaciones sobre el mismo.
1. **Apache Airflow** guarda el dataset procesado de *BodyFat* en **PostgreSQL**.
1. Mediante **MLFlow** se consume el dataset desde **PostgreSQL** con el fin de generar un modelo de *Machine Learning*.
1. Una vez generado el modelo, **MLFlow** lo guarda en **s3**.
1. Independientemente de los otros pasos, un *Usuario Final* realiza un llamado a la **REST API** para realizar una inferencia.
1. La **REST API** consume el modelo desde **s3**, realiza la inferencia, y le devuelve un resultado al *Usuario Final*.

### Entrega 1

- Servicios containerizados (*MLFlow*, *Airflow*, s3, *PostgreSQL*)
- Notebook loggeando en MLflow.
- *DAG* de procesamiento bÃ¡sico corriendo.
- *README* inicial.

### Entrega final

- DAG con hiperparÃ¡metros + promociÃ³n a Registry.
- Batch predict funcionando en Airflow.
- API usando modelo en Production.
- DocumentaciÃ³n + tests bÃ¡sicos.

### ðŸ“‚ Estructura de carpetas

```YAML
â”‚
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ dags/ # DAGs de Airflow
â”‚ â”‚ â”œâ”€â”€ train_model_dag.py
â”‚ â”‚ â””â”€â”€ batch_predict_dag.py
â”‚ â”œâ”€â”€ logs/ # Logs (excluidos del repo)
â”‚ â””â”€â”€ plugins/ # Operadores y hooks personalizados
â”‚
â”œâ”€â”€ api/ # Servicio de inferencia con FastAPI
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ schemas.py
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ diagrams/
â”‚
â”œâ”€â”€ dockerfiles/
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ mlflow/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ postgres/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ mlflow.sql
â”‚
â”œâ”€â”€ ml/ # CÃ³digo fuente del modelo
â”‚ â”œâ”€â”€ train.py # Entrenamiento y guardado del modelo
â”‚ â”œâ”€â”€ infer.py # PredicciÃ³n en batch
â”‚ â”œâ”€â”€ data_utils.py # Utilidades de carga y preprocesamiento
â”‚ â””â”€â”€ config.yaml # ConfiguraciÃ³n de hiperparÃ¡metros y paths
â”‚
â”œâ”€â”€ notebooks/ # ExploraciÃ³n y prototipado de modelos
â”‚ â”œâ”€â”€ body_fat_analysis.ipynb
â”‚ â””â”€â”€ mlflow_playground.ipynb
â”‚
â”œâ”€â”€ utilities/
â”‚   â””â”€â”€ scripts/ # scripts y otras utilidades
â”‚       â””â”€â”€ mlflow.sql
â”‚ â”œâ”€â”€ seed_minio.sh # Inicializar buckets y subir dataset
â”‚ â””â”€â”€ promote_model.py # Promover modelo en MLflow Registry
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yaml # OrquestaciÃ³n de servicios
â”œâ”€â”€ README.md # DocumentaciÃ³n del proyecto
â””â”€â”€ requirements.txt # DocumentaciÃ³n del proyecto
```

### Comandos Ãºtiles

***Requerimientos***

- *Docker* o *docker desktop* instalados

***Comandos***

- Para correr el proyecto:
  - `docker compose up`.
  - Opcionalmente `docker compose -f docker-compose.yaml up` para espeficiar el archivo compose.
- Para hacer build de las imÃ¡genes:
  - `docker compose build`.
- Para hacer build y correr el proyecto:
  - `docker compose up --build`
- Para hacer build y forzar la recreaciÃ³n de containers:
  - `docker compose up --build --force-recreate`
- Para borrar los containers:
  - `docker compose down`.
  - Opcionalmente `docker compose down -v` para borrar los ***volÃºmenes*** creados.
- Para hacer un *clean build*:
  - ***NOTA***: esto va a volver a crear las imÃ¡genes de docker, por lo cual puede *demorar mÃ¡s tiempo* de lo habitual.
  - `docker compose build --no-cache`
  - `docker compose up --force-recreate`
- ***RecomendaciÃ³n*** - Recrear ambiente local
  - `docker compose down -v`
  - Borrar las siguientes carpetas
    - `/airflow/dags/__pycache__`
    - `/minio_data`
  - Opcional: borrar las imÃ¡genes de docker
    - Ver el siguiente [post de Stack Overflow](https://stackoverflow.com/questions/44785585/how-can-i-delete-all-local-docker-images).
  - `docker compose up --build`
