# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements. See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. The ASF licenses this file
# to you under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied. See the License for the
# specific language governing permissions and limitations
# under the License.
#
# Modified by Facundo Lucianna and Rodrigo Lauro — Oct 2025
# WARNING: For local development only. Not for production.

x-airflow-common: &airflow-common
  build: './dockerfiles/airflow'
  image: ${AIRFLOW_IMAGE_NAME:-extending_airflow:latest}
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${PG_USER:-airflow}:${PG_PASSWORD:-airflow}@postgres/${PG_DATABASE:-airflow}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${PG_USER:-airflow}:${PG_PASSWORD:-airflow}@postgres/${PG_DATABASE:-airflow}
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__API_AUTH__JWT_SECRET: 'qt2Edq6LIRdv5k0DDznVOA=='
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'True'
    AIRFLOW__SECRETS__BACKEND: airflow.secrets.local_filesystem.LocalFilesystemBackend
    AIRFLOW__SECRETS__BACKEND_KWARGS: '{"variables_file_path": "/opt/secrets/variables.yaml", "connections_file_path": "/opt/secrets/connections.yaml"}'
    AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY:-minio}
    AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_ACCESS_KEY:-minio123}
    AWS_ENDPOINT_URL_S3: http://s3:9000
    MLFLOW_S3_ENDPOINT_URL: http://s3:9000
    AIRFLOW__API__WORKERS: '1'
    AIRFLOW__CORE__HOSTNAME_CALLABLE: socket.gethostname
  volumes:
    - ./data:/opt/data                  # ✅ carpeta local data montada correctamente
    - ${AIRFLOW_PROJ_DIR:-./airflow}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-./airflow}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-./airflow}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-./airflow}/plugins:/opt/airflow/plugins
    - ${AIRFLOW_PROJ_DIR:-./airflow}/secrets:/opt/secrets
    - ./ml:/opt/airflow/ml
    - ./utilities/scripts:/opt/airflow/utilities/scripts
  networks:
    - backend
  user: "0:0"
  depends_on: &airflow-common-depends-on
    postgres:
      condition: service_healthy
    redis:
      condition: service_healthy

services:
  mlflow:
    build: "./dockerfiles/mlflow"
    env_file: [.env]
    image: mlflow
    container_name: mlflow
    command: >
      mlflow server
      --backend-store-uri postgresql://${PG_USER:-airflow}:${PG_PASSWORD:-airflow}@postgres:${PG_PORT:-5432}/mlflow_db
      --default-artifact-root s3://${MLFLOW_BUCKET_NAME:-mlflow}/
      --host 0.0.0.0 --port 5000 --serve-artifacts
    volumes:
      - ./requirements.txt:/app/requirements.txt:ro
      - ./ml:/app/ml
      - ./notebooks:/app/notebooks
      - ./utilities:/app/utilities
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY:-minio}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_ACCESS_KEY:-minio123}
      - MLFLOW_S3_ENDPOINT_URL=http://s3:9000
    ports:
      - "0.0.0.0:5001:5000"
    depends_on:
      postgres:
        condition: service_healthy
      s3:
        condition: service_started
    networks: [backend]

  s3:
    image: minio/minio:latest
    container_name: s3
    env_file: [.env]
    command: server /data --console-address :9002
    environment:
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY:-minio}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_ACCESS_KEY:-minio123}
    ports:
      - "${MINIO_PORT:-9000}:9000"
      - "${MINIO_PORT_UI:-9002}:9002"
    volumes:
      - minio_data:/data
    networks: [backend]
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 60s
      timeout: 20s
      retries: 3

  createbuckets:
    image: minio/mc:latest
    env_file: [.env]
    container_name: s3_create_bucket
    depends_on: [s3]
    entrypoint: >
      /bin/sh -c '
      sleep 5;
      /usr/bin/mc alias set s3 http://s3:9000 ${MINIO_ACCESS_KEY:-minio} ${MINIO_SECRET_ACCESS_KEY:-minio123};
      /usr/bin/mc mb s3/${MLFLOW_BUCKET_NAME:-mlflow};
      /usr/bin/mc mb s3/${DATA_REPO_BUCKET_NAME:-data};
      exit 0;
      '
    networks: [backend]

  fastapi:
    restart: always
    build: "./dockerfiles/fastapi"
    image: backend_fastapi
    container_name: fastapi
    # depends_on:
    #   mlflow:
    #     condition: service_healthy
    #   airflow-apiserver:
    #     condition: service_healthy
    ports:
      - "${FASTAPI_PORT:-8800}:8800"
    networks:
      - backend
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY:-minio}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_ACCESS_KEY:-minio123}
      - MLFLOW_S3_ENDPOINT_URL=http://s3:9000
      - AWS_ENDPOINT_URL_S3=http://s3:9000
      - MLFLOW_TRACKING_URL=${MLFLOW_TRACKING_URL:-http://mlflow:5001}
    command: >
      uvicorn app:app 
      --host 0.0.0.0
      --port 8800
    volumes:
      - ./ml:/ml
      - ./utilities/scripts:/utilities/scripts      
    healthcheck:
      test: curl --include --request GET http://fastapi:8800/ || exit 1
      interval: 60s
      timeout: 10s
      start_period: 120s
      retries: 3

  postgres:
    restart: always
    build: "./dockerfiles/postgres"
    env_file: [.env]
    image: postgres-build
    container_name: postgres
    ports:
      - "${PG_PORT:-5432}:5432"
    networks: [backend]
    environment:
      - POSTGRES_DB=${PG_DATABASE:-airflow}
      - POSTGRES_USER=${PG_USER:-airflow}
      - POSTGRES_PASSWORD=${PG_PASSWORD:-airflow}
    volumes:
      - db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-p", "5432", "-U", "${PG_USER:-airflow}"]
      interval: 60s
      timeout: 20s
      retries: 3

  redis:
    image: valkey/valkey:8.1-bookworm
    env_file: [.env]
    expose: [6379]
    networks: [backend]
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  airflow-apiserver:
    <<: *airflow-common
    container_name: airflow-apiserver
    command: >
      bash -c "
      echo 'Esperando a Postgres...';
      until pg_isready -h postgres -p 5432 -U ${PG_USER:-airflow}; do sleep 2; done;
      echo 'Postgres listo. Iniciando API Server (Airflow 3)...';
      exec airflow api-server --port 8080 --host 0.0.0.0"
    ports: ["8080:8080"]
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8080/health | grep -q 'healthy' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 8
      start_period: 60s
    restart: always
    env_file: [.env]
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: >
      bash -c "
      until pg_isready -h postgres -p 5432 -U ${PG_USER:-airflow}; do sleep 2; done;
      airflow scheduler"
    restart: always
    env_file: [.env]
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    command: >
      bash -c "
      until pg_isready -h postgres -p 5432 -U ${PG_USER:-airflow}; do sleep 2; done;
      airflow dag-processor"
    restart: always
    env_file: [.env]
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: >
      bash -c "
      until pg_isready -h postgres -p 5432 -U ${PG_USER:-airflow}; do sleep 2; done;
      exec airflow celery worker"
    env_file: [.env]
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    restart: always

  airflow-triggerer:
    <<: *airflow-common
    command: >
      bash -c "
      until pg_isready -h postgres -p 5432 -U ${PG_USER:-airflow}; do sleep 2; done;
      airflow triggerer"
    restart: always
    env_file: [.env]
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-cli:
    <<: *airflow-common
    container_name: airflow_cli
    env_file: [.env]
    profiles: [debug]
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    command: ["bash", "-c", "airflow"]

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /opt/airflow/{logs,dags,plugins,config};
        /entrypoint airflow db upgrade;
        /entrypoint airflow users create \
            --username ${_AIRFLOW_WWW_USER_USERNAME:-airflow} \
            --password ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} \
            --firstname Rodrigo --lastname Lauro --role Admin --email admin@example.com;
        chown -R "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}" /opt/airflow;
    env_file: [.env]
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
    user: "0:0"

networks:
  backend:
    driver: bridge

volumes:
  db_data:
  minio_data:
